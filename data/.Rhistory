source('~/Documents/sjsu/Test.R')
source('~/Documents/sjsu/Test.R')
source('~/Documents/sjsu/Test.R')
1+1
getwd()
library(RGtk2)
install.packages("RGtk2", depen=T, type="source")
library(RGtk2)
library("RGtk2")
library("rattle")
setwd('/Users/kalpanatripathi/Documents/sjsu/AWSBackup/Flu-Prediction-master/data')
# Reading the data set
ILIData <- read.table('FinalDataSet_3_3.csv',sep=',',header=TRUE)
training<-ILIData[1:520,1:10]
# The 9 columns are attributes
training_9<-training[,1:9]
#The 10 column(ILI Positive Pct) is the result
training_10<-as.numeric(training[,10])
# Here we are not scaling the data as we did not see changes in the predictions.
training2 <- training_9
#Here we are dividing into training and test sets using random sampling.
index <- 1:nrow(training2)
training2[,10]<-training_10
testindex <- sample(index, trunc(length(index)/3))
testset <- training2[testindex, ]
trainingset <- training2[-testindex, ]
#Using Decision Trees
library(rattle)
library(rpart)
library(RColorBrewer)
rpart.model <- rpart(V10 ~ ., data = trainingset,control=rpart.control(maxdepth=5, minsplit=2))
prp(rpart.model)
fancyRpartPlot(rpart.model)
predictedata <- predict(rpart.model, trainingset[, -10])
error <- trainingset[,10] - predictedata
#Training Error using decision trees
(trainingError <- sqrt(sum(error*error))/(length(trainingset[,10])))
predictedata <- predict(rpart.model, testset[, -10])
error <- testset[,10] - predictedata
#Test Error using decision trees
(testError <- sqrt(sum(error*error))/(length(testset[,10])))
# Using SVM
svm.model <- svm(V10 ~ ., data = trainingset, cost = 100, gamma = 1)
predictedata<- predict(svm.model, testset[, -10])
error <- testset[,10] - predictedata
#Test Error using SVM before tuning
(testError <- sqrt(sum(error*error))/(length(testset[,10])))
# Tuning the model
obj <- tune(svm, V10~., data = trainingset,
ranges = list(gamma = 2^(-12:2), cost = 5^(2:4)),
extratuned = tune.control(sampling = "cross"))
obj$best.parameters
plot(obj,col.lab='green',col.main="red")
# Re-building the model using the best parameters
svm.model <- svm(V10 ~ ., data = trainingset, cost = 25, gamma = 0.125)
#plot(svm.model, trainingset, FluSpecimens ~ Region,color.palette = rainbow.colors)
predictedata<- predict(svm.model, trainingset[, -10])
error <- trainingset[,10] - predictedata
#Training Error using SVM after tuning
(trainingError <- sqrt(sum(error*error))/(length(trainingset[,10])))
#Test error
predictedata<- predict(svm.model, testset[, -10])
error <- testset[,10] - predictedata
#Test Error using SVM after tuning
(testError <- sqrt(sum(error*error))/(length(testset[,10])))
#Using Linear regression
lmILI <- lm(V10~., data = trainingset)
#Predicting the error using linear regression
lmFit <- predict(lmILI,newdata = testset[,-10])
error <- round(testset[,10]) - round(lmFit)
(testError <- sqrt(sum(error*error))/(length(testset[,10])))
#Error using linear regression
testError
plot(round(lmFit) ~ WeightedILIPCT, data = testset,ylab='% Flu Positive', xlab = '% Weighted ILI', main='Plot of % Flu Positive & % Weighted ILI',pch=c(10,10),bg="black",col.axis = 'blue', col.lab = 'darkgreen',col.main="darkred",col="red")
install.packages("RMySQL")
install.packages("DBI")
setwd('/Users/kalpanatripathi/Documents/sjsu/AWSBackup/Flu-Prediction-master/data')
# MYSQL database
library(RMySQL)
library(DBI)
mysqlconnection = dbConnect(MySQL(), user = 'root', password = '', dbname = 'ili_prediction',host = 'localhost')
dbListTables(mysqlconnection)
result = dbSendQuery(mysqlconnection,"select * from compute_target")
fluData = fetch(result, n = -1)
#From CSV
#fluData <- read.table('compute_target.csv',sep=',',header=TRUE,fill = TRUE)
#mean <- 1.288 # precomputed using flags
fluData$WeightedILIPCT
length(fluData$WeightedILIPCT)
ILI<-0
j<-1
mean<-mean(fluData$WeightedILIPCT,na.rm=TRUE)
SD1<-sd(fluData$WeightedILIPCT,na.rm=TRUE)+mean
SD2<-sd(fluData$WeightedILIPCT,na.rm=TRUE)*2+mean
SD3<-sd(fluData$WeightedILIPCT,na.rm=TRUE)*3+mean
SD4<-sd(fluData$WeightedILIPCT,na.rm=TRUE)*4+mean
SD5<-sd(fluData$WeightedILIPCT,na.rm=TRUE)*5+mean
SD6<-sd(fluData$WeightedILIPCT,na.rm=TRUE)*6+mean
SD7<-sd(fluData$WeightedILIPCT,na.rm=TRUE)*7+mean
SD8<-sd(fluData$WeightedILIPCT,na.rm=TRUE)*8+mean
SD9<-sd(fluData$WeightedILIPCT,na.rm=TRUE)*9+mean
for(i in fluData$WEIGHTED){
if(i<mean){
ILI[j]<-1
}
else if(i<SD1){
ILI[j]<-2
}
else if(i<SD2){
ILI[j]<-3
}
else if(i<SD3){
ILI[j]<-4
}
else if(i<SD4){
ILI[j]<-5
}
else if (i<SD5){
ILI[j]<-6
}
else if(i<SD6){
ILI[j]<-7
}
else if(i<SD7){
ILI[j]<-8
}
else if(i<SD7){
ILI[j]<-9
}
else if(i<SD8){
ILI[j]<-10
}
j<-j+1
}
m<-cbind(fluData,ILI)
write.table(m,file='fluData.csv',append=FALSE,sep=',',row.names=FALSE)
library(RMySQL)
library(DBI)
mysqlconnection = dbConnect(MySQL(), user = 'root', password = '', dbname = 'ili_prediction',host = 'localhost')
dbListTables(mysqlconnection)
result = dbSendQuery(mysqlconnection,"select * from compute_target")
fluData = fetch(result, n = -1)
#Team Datadrillers
setwd('/Users/kalpanatripathi/Documents/sjsu/AWSBackup/Flu-Prediction-master/data')
set.seed(pi)
rm(list=ls())
require(klaR)
library(MASS)
library(randomForest)
library(rpart.plot)
library(rattle)
library(RColorBrewer)
library(party)
library(partykit)
library(caret)
library(ggplot2)
library(DAAG)
library("parcor")
library(lars)
library(ridge)
library(class)
library(e1071)
# read data from csv database
#ILIData <- read.table('FinalDataSet_3_3.csv',sep=',',header=TRUE)
# MYSQL database
library(RMySQL)
library(DBI)
mysqlconnection = dbConnect(MySQL(), user = 'root', password = '', dbname = 'ili_prediction',host = 'localhost')
dbListTables(mysqlconnection)
result = dbSendQuery(mysqlconnection, "select * from finaldataset")
ILIData = fetch(result, n = -1)
train<-ILIData[1:520,1:11]
# use the first 9 columns into train_x
train_x<-train[,1:10]
# use the 11th col ILI Severity for result
train_y<-train[,11]
par(mar = rep(4, 4))
#Use Knn befor scaling data
out <- knn.cv(train_x,train_y,k=1)
print('Error with KNN before scaling:')
(1-sum(abs(train_y == out))/length(out))
train2 <- train_x
#Scaling data with for KNN
for(i in seq(from = 1, to = ncol(train_x))){
v = var(train_x[,i])
m = mean(train_x[,i])
train2[,i] <- (train_x[,i]-m)/sqrt(v)
}
(out <- knn.cv(train2,train_y,k=1))
print('RMSE Error with KNN with k=1 after executing scaling:')
(1-sum(train_y == out)/length(out))
Err <- rep(0,40)
for(jj in seq(from=1,to=40)){
out <- knn.cv(train2,train_y,k=jj)
Error <- 1-sum(abs(train_y == out))/length(out)
Err[jj] <- Error
}
print('Best values selected for k : ')
(which.min(Err))
print('Minimum error with KNN : ')
(min(Err))
# plot(Err,pch=c(15,15),col="red",xlab = 'K value', ylab='Error')
#Divinding dataset into train dataset & test dataset selected randomly so works just like cross validation
index <- 1:nrow(train2)
train2[,11]<-train_y
testIn <- sample(index, trunc(length(index)/3))
testset <- train2[testIn, ]
trainset <- train2[-testIn, ]
# Using decision trees
library(rpart)
rpart.model <- rpart(V11 ~ ., data = trainset,control=rpart.control(maxdepth=5, minsplit=2))
prp(rpart.model)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(rpart.model)
rpart.pred <- predict(rpart.model, testset[, -11])
print('RMSE Error for decision trees')
(1-(sum(rpart.pred == testset[,11])/length(testset[,11])))
# Using random forests
trainset[,11] <- as.factor(trainset[, 11])
modelRF <- randomForest(V11 ~ ., data = trainset)
modelRF
plot(modelRF, log="y")
legend("topright", legend=unique(trainset$V11), col=unique(trainset$V11), pch=19)
(predRF <- predict(modelRF, testset[, -11]))
print('RMSE Error for Random forest : ')
(1-(sum(predRF == testset[,11])/length(testset[,11])))
#Using SVM Radial basis function
svm.model <- svm(V11 ~ ., data = trainset, cost = 100, gamma = 1)
svm.pred <- predict(svm.model, testset[, -11])
print('RMSE Error for SVM before tuning')
(1-sum(svm.pred == testset[,11])/length(testset[,11]))
# tuning the SVM model
obj <- tune(svm, V11~., data = trainset,
ranges = list(gamma = 2^(-12:2), cost = 5^(2:4)),
tunecontrol = tune.control(sampling = "cross"))
obj$best.parameters
plot(obj)
# retry with the best calculated parameters after tuning
svm.model <- svm(V11 ~ ., data = trainset, method = "C-classification",kernel = "radial", cost = 625, gamma = 0.03125)
plot(svm.model, trainset, WeightedILIPCT ~ Region, svSymbol = 1, dataSymbol = 2,
color.palette = cm.colors)
svm.pred <- predict(svm.model, testset[, -11])
print('RMSE Error for SVM after tuning : ')
(1-sum(svm.pred == testset[,11])/length(testset[,11]))
#plot(svm.pred)
# Using Linear regression
lmflu <- lm(V11~., data = trainset)
print("Predicting error using above Linear Regression model")
lmFit <- predict(lmflu,newdata = testset[,-11])
accuracy<-sum(round(lmFit)== testset[,11])/length(testset[,11])
print('RMSE Error with linear regression : ')
(1-accuracy)
plot(round(lmFit) ~ WeightedILIPCT, data = testset,ylab='ILI Severity', xlab = '% Weighted ILI', main='Plot of ILI Severity & % Weighted ILI',pch=c(16,16),col="red")
#ridge regression
# library(glmnet)
# grid=10^seq(10,-2,length=100)
# X <- as.matrix(trainset[,-10])
# ridge.mod=glmnet(X,train2[,10],alpha=0,lambda=grid)
# pred<-predict(ridge.mod,s=50,type="coefficients")[1:10,]
# pred
# plot(ridge.mod)
# Visualize the flu data set for whole dataset
library(gridExtra)
colnames(ILIData)=c('Region','Year','Week','Season','Total No. Patients','No. Of Providers','PositiveWeightedPct','% Positive Unweighted','Total Influenza Test Specimens','InfluenzaPositive','ILISeverity','FluSeverity')
m<-qplot(Region,Week, data = ILIData, color = ILISeverity, size=InfluenzaPositive,na.rm = TRUE)
n<-m+scale_y_continuous(breaks=1:52)+scale_x_discrete(labels = c("CT, ME, MA.. ","NJ, NY..","DE, DC, MD..","AL, FL, GA..","IL, IN, MI..","AR, LA, NM..","IA, KS, MO..","CO, MT, ND..","AZ, CA,NV..","AK, ID, OR.."))
n
ggplot(ILIData, aes(x=PositiveWeightedPct)) +
geom_histogram(position="dodge", binwidth=3) +
scale_fill_brewer()+scale_x_discrete(breaks=0:8)
setwd('/Users/kalpanatripathi/Documents/sjsu/AWSBackup/Flu-Prediction-master/data')
# Reading the data set
#ILIData <- read.table('FinalDataSet_3_3.csv',sep=',',header=TRUE)
#MySQL Database
library(RMySQL)
library(DBI)
mysqlconnection = dbConnect(MySQL(), user = 'root', password = 'root', dbname = 'ili_prediction',host = 'localhost')
dbListTables(mysqlconnection)
result = dbSendQuery(mysqlconnection, "select * from finaldataset_3_3")
ILIData = fetch(result, n = -1)
training<-ILIData[1:520,1:10]
# The 9 columns are attributes
training_9<-training[,1:9]
#The 10 column(ILI Positive Pct) is the result
training_10<-as.numeric(training[,10])
# Here we are not scaling the data as we did not see changes in the predictions.
training2 <- training_9
#Here we are dividing into training and test sets using random sampling.
index <- 1:nrow(training2)
training2[,10]<-training_10
testindex <- sample(index, trunc(length(index)/3))
testset <- training2[testindex, ]
trainingset <- training2[-testindex, ]
#Using Decision Trees
library(rattle)
library(rpart)
library(RColorBrewer)
rpart.model <- rpart(V10 ~ ., data = trainingset,control=rpart.control(maxdepth=5, minsplit=2))
prp(rpart.model)
fancyRpartPlot(rpart.model)
predictedata <- predict(rpart.model, trainingset[, -10])
error <- trainingset[,10] - predictedata
#Training Error using decision trees
(trainingError <- sqrt(sum(error*error))/(length(trainingset[,10])))
predictedata <- predict(rpart.model, testset[, -10])
error <- testset[,10] - predictedata
#Test Error using decision trees
(testError <- sqrt(sum(error*error))/(length(testset[,10])))
# Using SVM
svm.model <- svm(V10 ~ ., data = trainingset, cost = 100, gamma = 1)
predictedata<- predict(svm.model, testset[, -10])
error <- testset[,10] - predictedata
#Test Error using SVM before tuning
(testError <- sqrt(sum(error*error))/(length(testset[,10])))
# Tuning the model
obj <- tune(svm, V10~., data = trainingset,
ranges = list(gamma = 2^(-12:2), cost = 5^(2:4)),
extratuned = tune.control(sampling = "cross"))
obj$best.parameters
plot(obj,col.lab='green',col.main="red")
# Re-building the model using the best parameters
svm.model <- svm(V10 ~ ., data = trainingset, cost = 25, gamma = 0.125)
#plot(svm.model, trainingset, FluSpecimens ~ Region,color.palette = rainbow.colors)
predictedata<- predict(svm.model, trainingset[, -10])
error <- trainingset[,10] - predictedata
#Training Error using SVM after tuning
(trainingError <- sqrt(sum(error*error))/(length(trainingset[,10])))
#Test error
predictedata<- predict(svm.model, testset[, -10])
error <- testset[,10] - predictedata
#Test Error using SVM after tuning
(testError <- sqrt(sum(error*error))/(length(testset[,10])))
#Using Linear regression
lmILI <- lm(V10~., data = trainingset)
#Predicting the error using linear regression
lmFit <- predict(lmILI,newdata = testset[,-10])
error <- round(testset[,10]) - round(lmFit)
(testError <- sqrt(sum(error*error))/(length(testset[,10])))
#Error using linear regression
testError
plot(round(lmFit) ~ WeightedILIPCT, data = testset,ylab='% Flu Positive', xlab = '% Weighted ILI', main='Plot of % Flu Positive & % Weighted ILI',pch=c(10,10),bg="black",col.axis = 'blue', col.lab = 'darkgreen',col.main="darkred",col="red")
install.packages("rattle")
#Team Datadrillers
setwd('/Users/kalpanatripathi/Documents/sjsu/AWSBackup/Flu-Prediction-master/data')
set.seed(pi)
rm(list=ls())
require(klaR)
library(MASS)
library(randomForest)
library(rpart.plot)
library(rattle)
library(RColorBrewer)
library(party)
library(partykit)
library(caret)
library(ggplot2)
library(DAAG)
library("parcor")
library(lars)
library(ridge)
library(class)
library(e1071)
# read data from csv database
#ILIData <- read.table('FinalDataSet_3_3.csv',sep=',',header=TRUE)
# MYSQL database
library(RMySQL)
library(DBI)
mysqlconnection = dbConnect(MySQL(), user = 'root', password = '', dbname = 'ili_prediction',host = 'localhost')
dbListTables(mysqlconnection)
result = dbSendQuery(mysqlconnection, "select * from finaldataset")
ILIData = fetch(result, n = -1)
train<-ILIData[1:520,1:11]
# use the first 9 columns into train_x
train_x<-train[,1:10]
# use the 11th col ILI Severity for result
train_y<-train[,11]
par(mar = rep(4, 4))
#Use Knn befor scaling data
out <- knn.cv(train_x,train_y,k=1)
print('Error with KNN before scaling:')
(1-sum(abs(train_y == out))/length(out))
train2 <- train_x
#Scaling data with for KNN
for(i in seq(from = 1, to = ncol(train_x))){
v = var(train_x[,i])
m = mean(train_x[,i])
train2[,i] <- (train_x[,i]-m)/sqrt(v)
}
(out <- knn.cv(train2,train_y,k=1))
print('RMSE Error with KNN with k=1 after executing scaling:')
(1-sum(train_y == out)/length(out))
Err <- rep(0,40)
for(jj in seq(from=1,to=40)){
out <- knn.cv(train2,train_y,k=jj)
Error <- 1-sum(abs(train_y == out))/length(out)
Err[jj] <- Error
}
print('Best values selected for k : ')
(which.min(Err))
print('Minimum error with KNN : ')
(min(Err))
# plot(Err,pch=c(15,15),col="red",xlab = 'K value', ylab='Error')
#Divinding dataset into train dataset & test dataset selected randomly so works just like cross validation
index <- 1:nrow(train2)
train2[,11]<-train_y
testIn <- sample(index, trunc(length(index)/3))
testset <- train2[testIn, ]
trainset <- train2[-testIn, ]
# Using decision trees
library(rpart)
rpart.model <- rpart(V11 ~ ., data = trainset,control=rpart.control(maxdepth=5, minsplit=2))
prp(rpart.model)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(rpart.model)
rpart.pred <- predict(rpart.model, testset[, -11])
print('RMSE Error for decision trees')
(1-(sum(rpart.pred == testset[,11])/length(testset[,11])))
# Using random forests
trainset[,11] <- as.factor(trainset[, 11])
modelRF <- randomForest(V11 ~ ., data = trainset)
modelRF
plot(modelRF, log="y")
legend("topright", legend=unique(trainset$V11), col=unique(trainset$V11), pch=19)
(predRF <- predict(modelRF, testset[, -11]))
print('RMSE Error for Random forest : ')
(1-(sum(predRF == testset[,11])/length(testset[,11])))
#Using SVM Radial basis function
svm.model <- svm(V11 ~ ., data = trainset, cost = 100, gamma = 1)
svm.pred <- predict(svm.model, testset[, -11])
print('RMSE Error for SVM before tuning')
(1-sum(svm.pred == testset[,11])/length(testset[,11]))
# tuning the SVM model
obj <- tune(svm, V11~., data = trainset,
ranges = list(gamma = 2^(-12:2), cost = 5^(2:4)),
tunecontrol = tune.control(sampling = "cross"))
obj$best.parameters
plot(obj)
# retry with the best calculated parameters after tuning
svm.model <- svm(V11 ~ ., data = trainset, method = "C-classification",kernel = "radial", cost = 625, gamma = 0.03125)
plot(svm.model, trainset, WeightedILIPCT ~ Region, svSymbol = 1, dataSymbol = 2,
color.palette = cm.colors)
svm.pred <- predict(svm.model, testset[, -11])
print('RMSE Error for SVM after tuning : ')
(1-sum(svm.pred == testset[,11])/length(testset[,11]))
#plot(svm.pred)
# Using Linear regression
lmflu <- lm(V11~., data = trainset)
print("Predicting error using above Linear Regression model")
lmFit <- predict(lmflu,newdata = testset[,-11])
accuracy<-sum(round(lmFit)== testset[,11])/length(testset[,11])
print('RMSE Error with linear regression : ')
(1-accuracy)
plot(round(lmFit) ~ WeightedILIPCT, data = testset,ylab='ILI Severity', xlab = '% Weighted ILI', main='Plot of ILI Severity & % Weighted ILI',pch=c(16,16),col="red")
#ridge regression
# library(glmnet)
# grid=10^seq(10,-2,length=100)
# X <- as.matrix(trainset[,-10])
# ridge.mod=glmnet(X,train2[,10],alpha=0,lambda=grid)
# pred<-predict(ridge.mod,s=50,type="coefficients")[1:10,]
# pred
# plot(ridge.mod)
# Visualize the flu data set for whole dataset
library(gridExtra)
colnames(ILIData)=c('Region','Year','Week','Season','Total No. Patients','No. Of Providers','PositiveWeightedPct','% Positive Unweighted','Total Influenza Test Specimens','InfluenzaPositive','ILISeverity','FluSeverity')
m<-qplot(Region,Week, data = ILIData, color = ILISeverity, size=InfluenzaPositive,na.rm = TRUE)
n<-m+scale_y_continuous(breaks=1:52)+scale_x_discrete(labels = c("CT, ME, MA.. ","NJ, NY..","DE, DC, MD..","AL, FL, GA..","IL, IN, MI..","AR, LA, NM..","IA, KS, MO..","CO, MT, ND..","AZ, CA,NV..","AK, ID, OR.."))
n
ggplot(ILIData, aes(x=PositiveWeightedPct)) +
geom_histogram(position="dodge", binwidth=3) +
scale_fill_brewer()+scale_x_discrete(breaks=0:8)
shiny::runApp('~/Documents/sjsu/AWSBackup/Flu-Prediction-master')
